{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given input text in source language, identify terminology and dialogue styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = \"\"\n",
    "with open(\"example2.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        source_text += line.strip()\n",
    "        source_text += '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "GPT_MODEL = \"gpt-4\"\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_source_text(source_text: str, max_tokens: int = 8192) -> list[str]:\n",
    "    \"\"\"Chunk the source text into chunks with a maximum number of tokens.\"\"\"\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    \n",
    "    for line in source_text.split('\\n'):\n",
    "        line_token_count = num_tokens(line)\n",
    "        \n",
    "        if line_token_count > max_tokens:\n",
    "            raise ValueError(f\"Line with more than {max_tokens} tokens\")\n",
    "        \n",
    "        chunk_token_count = num_tokens(chunk)\n",
    "\n",
    "        cut_here = (line.strip() == \"\" and chunk_token_count > max_tokens/8) \\\n",
    "            or (chunk_token_count + line_token_count > max_tokens)\n",
    "        \n",
    "        if cut_here:\n",
    "            chunks.append(chunk)\n",
    "            chunk = line + '\\n'\n",
    "        else:\n",
    "            chunk += line + '\\n'\n",
    "    \n",
    "    chunks.append(chunk)  # Append the last chunk\n",
    "    return chunks\n",
    "\n",
    "# chunks = chunk_source_text(source_text)\n",
    "# for chunk in chunks:\n",
    "#     print(f\"Chunk with {num_tokens(chunk)} tokens:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_MANGA_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/freyazhu/Library/Caches/pypoetry/virtualenvs/ragathon-manga-0usXnPYr-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "gemini = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_1 = \"You are localizing an anime light novel. Identify named entities (such as proper nouns and unique terminologies) in the provided text passage and suggest English translations.\"\n",
    "instructions_2 = \"Do not redefine terms already in the glossary. \"\n",
    "instructions_3 = \"Output in JSON format: `{ \\\"original_word\\\": \\\"translated_word\\\", ... }`.\"\n",
    "\n",
    "def get_named_entities_from_gpt3(text: str, glossary: dict = None) -> str:\n",
    "    \"\"\"Ask GPT model to extract named entities from text.\"\"\"\n",
    "    glossary_instructions = instructions_2 if glossary else \"\"\n",
    "    instructions = instructions_1 + glossary_instructions + instructions_3\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "\n",
    "    if glossary:\n",
    "        sub_glossary = {k: v for k, v in glossary.items() if k in text}\n",
    "        sub_glossary_string = str(sub_glossary)\n",
    "        messages.insert(1, {\"role\": \"assistant\", \"content\": f\"Current glossary: {sub_glossary_string}\"})\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    completion_message = completion.choices[0].message.content\n",
    "    return completion_message\n",
    "\n",
    "def get_named_entities_from_gpt4(text: str, glossary: dict = None) -> str:\n",
    "    \"\"\"Ask GPT model to extract named entities from text.\"\"\"\n",
    "    glossary_instructions = instructions_2 if glossary else \"\"\n",
    "    instructions = instructions_1 + glossary_instructions + instructions_3\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "\n",
    "    if glossary:\n",
    "        sub_glossary = {k: v for k, v in glossary.items() if k in text}\n",
    "        sub_glossary_string = str(sub_glossary)\n",
    "        messages.insert(1, {\"role\": \"assistant\", \"content\": f\"Current glossary: {sub_glossary_string}\"})\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=messages,\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    completion_message = completion.choices[0].message.content\n",
    "    return completion_message\n",
    "\n",
    "def get_named_entities_from_gemini(text: str, glossary: dict = None) -> str:\n",
    "    glossary_instructions = instructions_2 if glossary else \"\"\n",
    "    instructions = instructions_1 + glossary_instructions + instructions_3\n",
    "\n",
    "    if glossary:\n",
    "        sub_glossary = {k: v for k, v in glossary.items() if k in text}\n",
    "        sub_glossary_string = str(sub_glossary)\n",
    "        instructions += f\" Current glossary: {sub_glossary_string}\"\n",
    "    \n",
    "    message = f\"{instructions}\\n\\nText:\\n```{text}```\"\n",
    "\n",
    "    response = gemini.generate_content(message)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def update_glossary(glossary: dict, new_terms: dict):\n",
    "    for k, v in new_terms.items():\n",
    "        if len(k) > 10: # too long\n",
    "            continue\n",
    "        glossary[k] = v\n",
    "\n",
    "def build_glossary_pipeline_gpt3(source_text: str) -> dict:\n",
    "    \"\"\"Extract named entities from source text and build terminology dictionary.\"\"\"\n",
    "    glossary = {}\n",
    "    chunks = chunk_source_text(source_text)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        named_entities = get_named_entities_from_gpt3(chunk, glossary)\n",
    "        update_glossary(glossary, json.loads(named_entities))\n",
    "        print(str(glossary), end='\\r')\n",
    "\n",
    "    return glossary\n",
    "\n",
    "def build_glossary_pipeline_gemini(source_text: str) -> dict:\n",
    "    glossary = {}\n",
    "    chunks = chunk_source_text(source_text)\n",
    "\n",
    "    for text in chunks:\n",
    "        glossary_instructions = instructions_2 if glossary else \"\"\n",
    "        instructions = instructions_1 + glossary_instructions + instructions_3\n",
    "\n",
    "        if glossary:\n",
    "            sub_glossary = {k: v for k, v in glossary.items() if k in text}\n",
    "            sub_glossary_string = str(sub_glossary)\n",
    "            instructions += f\" Current glossary: {sub_glossary_string}\"\n",
    "        \n",
    "        message = f\"{instructions}\\n\\nText:\\n```{text}```\"\n",
    "        \n",
    "        chat = gemini.start_chat()\n",
    "        response = chat.send_message(\n",
    "            message,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        match = re.search(r\"{.*}\", response.text, re.DOTALL)\n",
    "        if match:\n",
    "            update_glossary(glossary, json.loads(match.group()))\n",
    "            print(str(glossary), end='\\r')\n",
    "\n",
    "    return glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"中考\": \"entrance exam\",\n",
      "  \"城原千太郎\": \"Shirohara Sentaro\",\n",
      "  \"雾乃雫\": \"Kirino Shizuku\",\n",
      "  \"樱\": \"Sakura\",\n",
      "  \"石田\": \"Ishida\",\n",
      "  \"BUNNYS\": \"Bunny's\",\n",
      "  \"神奈川县逗子海岸店\": \"Kanagawa Prefecture Zushi Coast Store\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_source_text(source_text)\n",
    "entities = get_named_entities_from_gemini(chunks[0])\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"城原千太郎\": \"Jōhara Chitarō\",\n",
      "  \"雾乃雫\": \"Kirino Shizuku\",\n",
      "  \"樱\": \"Sakura\",\n",
      "  \"石田\": \"Ishida\",\n",
      "  \"BUNNYS\": \"BUNNYS\",\n",
      "  \"神奈川县逗子海岸店\": \"Kanagawa Prefecture Zushi Coast Store\",\n",
      "  \"辻桥高中\": \"Tsujibashi High School\",\n",
      "  \"监督\": \"Director\",\n",
      "  \"棒球教练\": \"Baseball Coach\",\n",
      "  \"工程监理\": \"Construction Supervisor\",\n",
      "  \"旧视听室\": \"Old Audio-Visual Room\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_source_text(source_text, max_tokens=120000)\n",
    "entities = get_named_entities_from_gpt4(chunks[0])\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'城原千太郎': 'Chitara Johara', '雾乃雫': 'Kasumi Kirino', '电影导演': 'film director', '超级帅气角色': 'super handsome character', '樱': 'Sakura', '石田': 'Ishida', '中考': 'entrance exam', '学妹': 'junior student', '自行车': 'bicycle', '学长': 'senior student', '神奈川县逗子海岸店': 'Zushi Coast branch in Kanagawa Prefecture', '四月中旬': 'mid-April', '海风': 'sea breeze', '哈欠': 'yawn', '神奈川县立辻桥高中': 'Kanagawa Tsujiki High School', 'BUNNYS': 'BUNNYS', '芭菲': 'parfait', '喵喵收藏品': 'Meow Meow collectible', '辻桥高中': 'Tsujiki High School', 'A型血': 'blood type A', '归宅部': 'After-School Club', '家庭餐厅': 'family restaurant', '儿童午餐': \"children's lunch\", '监督': 'supervisor', '导演': 'director', '棒球教练': 'baseball coach', '工程监理': 'engineering supervisor', '英语笔记': 'English notes', '二年级': 'second year', '精通七国语言': 'fluent in seven languages', '不及格': 'failing grade', '黑暗': 'darkness', '玻璃杯': 'glass', '店长': 'store manager', '炒鱿鱼': 'get fired', '城原同学': 'Hirohara', '鹰野店长': 'Takanashi', '逗子海岸店': 'Zushi Coast Store', '逗子的人鱼': 'Zushi Mermaid', '逗子的鱼人': 'Zushi Fishman', '微观管理': 'micro-management', '经理培训': 'manager training', '给游戏氪金': 'spend money on in-game purchases', '春季抽卡': 'spring gacha', '冬眠': 'hibernation', '灯里': 'Tomari', '葛格': 'Gaku', '番茄酱': 'ketchup', '小鸟': 'little bird', '闲人免进': 'no entry for idle people', '训斥场所': 'reprimand place', '妹妹': 'younger sister', '手机': 'smartphone', '五岁': 'five years old', '演员': 'actor', '视听室': 'audiovisual room', '打工': 'part-time job', '社会规则': 'social rules', '约定': 'promise', '地狱': 'hell', '少女': 'girl'}\n"
     ]
    }
   ],
   "source": [
    "glossary = build_glossary_pipeline_gpt3(source_text)\n",
    "print(glossary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'城原千太郎': 'Chihara Sentaro', '雾乃雫': 'Kirino Shizuku', '樱': 'Sakura', '石田': 'Ishida', 'BUNNYS': 'BUNNYS', '神奈川县逗子海岸店': 'Kanagawa Prefecture Zushi Coast Store', '神奈川县立辻桥高中': 'Kanagawa Prefectural Tsujihashi High School', '辻桥高中': 'Tsujihashi High School', '辻桥高中二年B班': 'Tsujihashi High School, Class 2-B', '城原千太郎学长': 'Senior Chihara Sentaro', '监督': 'Supervisor', '城原同——学': 'Shirohara-kun', '鹰野': 'Takano', '逗子海岸': 'Zushi Kaigan', '逗子的人鱼': 'Mermaid of Zushi', '逗子的鱼人': 'Fishman of Zushi', '微观管理': 'Micromanagement', '葛格': 'Onii-chan', '灯里': 'Hotaru', '光之美少女角色扮演': 'PreCure cosplay', '危机(crisis)': 'crisis', '训斥场所': 'scolding place', '演员': 'Actor', '旧视听室': 'Old Audio-Visual Room', '学妹': 'Junior', '店长': 'Manager'}\n"
     ]
    }
   ],
   "source": [
    "glossary = build_glossary_pipeline_gemini(source_text)\n",
    "print(glossary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manga-kernel",
   "language": "python",
   "name": "manga-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
